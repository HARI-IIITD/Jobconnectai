{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304b7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03958c",
   "metadata": {},
   "source": [
    "# Pipeline:\n",
    "* Data acquisition - Extracting data from raw data.\n",
    "* Text Extraction and CLeanup - Extracting the texts (like in json format) and cleaning the grammar mistakes and extra lines\n",
    "* Tokenization - breaking the texts into appropriate parts and followed by breaking the broke sentences into appropriate words\n",
    "* Stemming - using some common defined rules to come up with base words\n",
    "* Lemmatization - A follow up of `stemming` to find the base word in the sentence\n",
    "* Vectorization - converting the text data into numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ae558d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc1 = nlp(\"Dr. Strange loves pav bhaji of Mumbai. Hulk loves Misal of Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "203f537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of Mumbai.\n",
      "Hulk loves Misal of Pune\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc1.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03814f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji as it costs only 25 per plate.\n",
      "Hi how are you?\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc2 = nlp2(\"Dr. Strange loves pav bhaji as it costs only 25 per plate. Hi how are you?\")\n",
    "doc3 = nlp2(\"\\\"let's go to N.Y.!\\\"\")\n",
    "for sentence in doc2.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6426a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AI.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11c94129",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "022bed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908\n",
      "321\n",
      "Natural language processing, often abbreviated as NLP, is one of the most active areas of artificial intelligence. It focuses on enabling computers to understand, interpret, and generate human language. From chatbots and voice assistants to translation systems and sentiment analysis tools, NLP has become deeply embedded in our daily lives. The challenge lies in dealing with ambiguities, variations, and the complex structure of natural language.\n",
      "\n",
      "One of the most popular NLP frameworks is spaCy, an open-source Python library designed for industrial-strength NLP tasks. It provides efficient tokenization, part-of-speech tagging, named entity recognition, and dependency parsing out of the box. Unlike many research-oriented libraries, spaCy is built for production â€” focusing on performance, accuracy, and scalability. Its pre-trained models make it possible to process large volumes of text quickly while maintaining linguistic accuracy.\n",
      "\n",
      "Machine learning models, particularly deep learning architectures, have further advanced NLP capabilities. Transformer-based models like BERT and GPT have significantly improved contextual understanding of text. These models learn the relationships between words in a sentence and can generate high-quality predictions for tasks like question answering, summarization, and text classification. The integration of such models with libraries like spaCy allows developers to combine deep contextual embeddings with robust linguistic pipelines.\n",
      "\n",
      "However, NLP is still an evolving field. Challenges such as bias in datasets, multilingual processing, and understanding sarcasm remain open problems. As AI systems are deployed in more sensitive applications like legal or medical domains, ethical considerations become crucial. Researchers and engineers continue to refine models, datasets, and algorithms to make NLP more fair, transparent, and universally accessible.\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(doc))\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ebc7b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "a\n",
      "l\n",
      " \n",
      "l\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for token in text[0:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "999cc98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      ",\n",
      "often\n",
      "abbreviated\n",
      "as\n",
      "NLP\n",
      ",\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "for token in doc[0:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a86bf2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing, often abbreviated as NLP, is one of the most active areas of artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "sentence1 = list(doc.sents)[0]\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9cb9c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n"
     ]
    }
   ],
   "source": [
    "token2 = sentence1[2]\n",
    "token1 = sentence1[1]\n",
    "token0 = sentence1[0]\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88a47bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'processing'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9b56471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ","
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.right_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "366db1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Natural"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.left_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31d40779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(token2.ent_type)\n",
    "print(token1.ent_type)\n",
    "print(token0.ent_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05d9e79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.ent_iob_\n",
    "token1.ent_iob_\n",
    "token0.ent_iob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d4905a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'processing'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.lemma_\n",
    "token2.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c8a56732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number=Sing"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06b89605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token2)\n",
    "token2.pos_ # part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dcaf8a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP ORG\n",
      "NLP ORG\n",
      "One CARDINAL\n",
      "NLP ORG\n",
      "NLP ORG\n",
      "NLP ORG\n",
      "BERT ORG\n",
      "GPT ORG\n",
      "NLP ORG\n",
      "AI ORG\n",
      "NLP ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents: # will return a tuple of all named entities recognized in the document\n",
    "    print(ent.text, ent.label_) # entity type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb95700",
   "metadata": {},
   "source": [
    "ORG - any organization, company, institution or group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b5f0911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aayush GPE\n",
      "Google ORG\n",
      "Pinki GPE\n",
      "Lionel Messi PERSON\n",
      "Narendra Modi PERSON\n",
      "Aditya PERSON\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"I am Aayush. I would love to work in Google and with Pinki. ALso My role model is Lionel Messi and Narendra Modi. I am Aditya\")\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d86d7",
   "metadata": {},
   "source": [
    "Aayush - GPE\n",
    "Aryan - NORP\n",
    "google - ORG\n",
    "Pinki - GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9e3b9e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aayushjha3@gmail.com, piyushj3443@gmail.com\n",
      "piyushj3443@gmail.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = \"aayushjha3@gmail.com, piyushj3443@gmail.com\"\n",
    "\n",
    "doc_mail = nlp(trial)\n",
    "\n",
    "for sent in doc_mail.sents:\n",
    "    print(sent)\n",
    "\n",
    "mail_token0 = list(doc_mail)[0]\n",
    "mail_token2 = list(doc_mail)[2]\n",
    "print(mail_token2)\n",
    "mail_token2.like_email"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
